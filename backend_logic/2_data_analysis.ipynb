{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# üîç Phase 2: Exploratory Data Analysis (EDA) & Quality Audit\n",
    "**Module:** Backend Logic  \n",
    "**Goal:** Inspect the \"Silver\" data in OneLake for statistical anomalies, outliers, and artificial capping.\n",
    "\n",
    "### **ü•à Silver Layer**\n",
    "Now that our data is safely stored in **Microsoft Fabric (OneLake)**, we shift our focus to **Data Science**. Before feeding this data into any AI agent or ML model, we must perform a \"Health Check.\"\n",
    "\n",
    "During the design phase, we identified three critical risks that could ruin an AI model:\n",
    "1.  **Artificial Capping:** Does the survival data suddenly stop at 60 months? (Indicates study duration limits, not actual patient survival).\n",
    "2.  **Extreme Outliers:** Are there ages > 120 or tumor sizes that are physically impossible? (Indicates data entry errors).\n",
    "3.  **Distribution Skew:** Is the data normal or skewed? (Determines which scaling algorithms we need later).\n",
    "\n",
    "In this notebook, we connect our local VS Code environment directly to the Cloud Data Warehouse to visualize these truths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Data Science & Database libraries\n",
    "# %pip ensures they install to the current Jupyter Kernel\n",
    "%pip install sqlalchemy seaborn matplotlib pyodbc pandas python-dotenv azure-identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### üîê Step 1: Secure Connection to Fabric SQL Endpoint\n",
    "\n",
    "We are connecting to the **SQL Analytics Endpoint** of our Lakehouse. This allows us to treat the Delta Tables in OneLake just like a standard SQL Server database.\n",
    "\n",
    "### **Concept Guide: Passwordless Authentication**\n",
    "Notice we are **NOT** using a username/password. We use `DefaultAzureCredential`.\n",
    "* This uses your local Azure CLI login token (`az login`).\n",
    "* It is the industry standard for **Zero Trust Security**. We never hardcode secrets in our code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Manual Configuration: SQL Connection String\n",
    "\n",
    "**Context:**\n",
    "We attempted to retrieve the SQL Connection String programmatically via the Fabric API. However, the API sometimes creates a \"provisioning delay\" where the connection details are hidden for security reasons until the endpoint is fully \"warmed up.\"\n",
    "\n",
    "**The \"Unblock\" Strategy:**\n",
    "In a production environment, we would implement a \"wait-and-retry\" loop. For this demo, we will use the **15-Minute Rule**: *If automation takes longer than 15 minutes to debug for a one-time setup, do it manually.*\n",
    "\n",
    "**Instructions:**\n",
    "1.  Open your [Microsoft Fabric Workspace](https://app.fabric.microsoft.com) in your browser.\n",
    "2.  Locate the item named **`OncologyRawData`**.\n",
    "    * ‚ö†Ô∏è **Crucial:** Click the item with the **SQL Endpoint icon** (looks like a database cylinder with a magnifying glass), *not* the Lakehouse icon.\n",
    "3.  In the top-right corner, click the **Gear Icon (Settings)**.\n",
    "4.  Locate the field **\"SQL connection string\"** and copy the value.\n",
    "    * *Format:* `x12345-guid.datawarehouse.fabric.microsoft.com`\n",
    "5.  Open the `.env` file in your project root and paste it:\n",
    "    ```bash\n",
    "    SQL_CONNECTION_STRING=your-copied-string-here.datawarehouse.fabric.microsoft.com\n",
    "    ```\n",
    "\n",
    "> **Note:** Do not add `https://` or `jdbc:` prefixes. Just the server address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure login if not already logged in (Interactive)\n",
    "!az login --tenant 50ecd9d1-4e73-43d1-8665-fc88114ccfc7\n",
    "!az account set --subscription 47def3  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "def fetch_sql_connection_string_detailed():\n",
    "    workspace_id = os.getenv(\"AZURE_WORKSPACE_ID\")\n",
    "    lakehouse_name = os.getenv(\"LAKEHOUSE_NAME\")\n",
    "    \n",
    "    # Authenticate\n",
    "    credential = DefaultAzureCredential()\n",
    "    token = credential.get_token(\"https://api.fabric.microsoft.com/.default\")\n",
    "    headers = {\"Authorization\": f\"Bearer {token.token}\"}\n",
    "\n",
    "    # 1. Find the Item ID first (we know this works)\n",
    "    list_url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/items\"\n",
    "    list_response = requests.get(list_url, headers=headers)\n",
    "    \n",
    "    if list_response.status_code == 200:\n",
    "        items = list_response.json().get(\"value\", [])\n",
    "        # Find the ID of the SQL Endpoint\n",
    "        target = next((i for i in items if i[\"type\"] == \"SQLEndpoint\" and i[\"displayName\"] == lakehouse_name), None)\n",
    "        \n",
    "        if target:\n",
    "            item_id = target[\"id\"]\n",
    "            print(f\"üìç Found SQL Endpoint ID: {item_id}\")\n",
    "            \n",
    "            # 2. THE FIX: Query the specific Item URL to get full properties\n",
    "            detail_url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/items/{item_id}\"\n",
    "            detail_response = requests.get(detail_url, headers=headers)\n",
    "            \n",
    "            if detail_response.status_code == 200:\n",
    "                full_details = detail_response.json()\n",
    "                conn_str = full_details.get(\"properties\", {}).get(\"connectionString\")\n",
    "                \n",
    "                if conn_str:\n",
    "                    print(\"\\n‚úÖ SUCCESS! Connection String Retrieved:\")\n",
    "                    print(f'SQL_CONNECTION_STRING=\"{conn_str}\"')\n",
    "                    print(\"\\nüìã Action: Update your .env file with this value.\")\n",
    "                else:\n",
    "                    print(\"‚ùå Property still missing. The endpoint might be in a 'Provisioning' state.\")\n",
    "                    print(\"Debug Info:\", json.dumps(full_details, indent=2))\n",
    "            else:\n",
    "                print(f\"‚ùå Failed to get details. Status: {detail_response.status_code}\")\n",
    "        else:\n",
    "            print(f\"‚ùå SQL Endpoint '{lakehouse_name}' not found in list.\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to list items. Status: {list_response.status_code}\")\n",
    "\n",
    "fetch_sql_connection_string_detailed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# 1. Force find the .env file\n",
    "env_path = find_dotenv()\n",
    "\n",
    "print(\"--- DIAGNOSTICS ---\")\n",
    "if env_path == \"\":\n",
    "    print(\"‚ùå ERROR: No .env file found in this folder or any parent folder.\")\n",
    "    print(\"   Action: Ensure a file named exactly '.env' is in your project root folder.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found .env file at: {env_path}\")\n",
    "    \n",
    "    # 2. Force Reload (override=True is critical if you just edited the file)\n",
    "    load_dotenv(env_path, override=True)\n",
    "    \n",
    "    # 3. Check the specific key\n",
    "    conn_str = os.getenv(\"SQL_CONNECTION_STRING\")\n",
    "    \n",
    "    if conn_str:\n",
    "        print(\"‚úÖ SQL_CONNECTION_STRING is loaded.\")\n",
    "        print(f\"   Value starts with: {conn_str[:10]}...\") # Print first 10 chars only for security\n",
    "    else:\n",
    "        print(\"‚ùå SQL_CONNECTION_STRING is missing or None.\")\n",
    "        print(\"   Action: Open your .env file and ensure it looks like this:\")\n",
    "        print('   SQL_CONNECTION_STRING=\"x123...datawarehouse.fabric.microsoft.com\"')\n",
    "\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# 1. Load Config\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "SERVER = os.getenv(\"SQL_CONNECTION_STRING\")\n",
    "DATABASE = os.getenv(\"LAKEHOUSE_NAME\")\n",
    "DRIVER = os.getenv(\"ODBC_DRIVER_NAME\", \"ODBC Driver 18 for SQL Server\")\n",
    "\n",
    "print(f\"üîå Connecting to: {SERVER}...\")\n",
    "\n",
    "try:\n",
    "    # 2. Get Access Token\n",
    "    credential = DefaultAzureCredential()\n",
    "    token_obj = credential.get_token(\"https://database.windows.net/.default\")\n",
    "    token_str = token_obj.token\n",
    "\n",
    "    # Convert string to UTF-16LE bytes\n",
    "    token_bytes = token_str.encode(\"utf-16-le\")\n",
    "    token_struct = struct.pack(\"<I\", len(token_bytes)) + token_bytes\n",
    "\n",
    "    # 3. Build Raw Connection String\n",
    "    raw_connection_string = (\n",
    "        f\"Driver={{{DRIVER}}};\"\n",
    "        f\"Server={SERVER},1433;\"\n",
    "        f\"Database={DATABASE};\"\n",
    "        f\"Encrypt=yes;\"\n",
    "        f\"TrustServerCertificate=no;\"\n",
    "        f\"Connection Timeout=30;\"\n",
    "    )\n",
    "\n",
    "    # 4. Create Engine (THE FIX IS HERE)\n",
    "    # We define a standard function instead of a complex lambda\n",
    "    def get_connection():\n",
    "        # Return the CONNECTION object directly (do not call .cursor())\n",
    "        return __import__(\"pyodbc\").connect(raw_connection_string, attrs_before={1256: token_struct})\n",
    "\n",
    "    engine = create_engine(\"mssql+pyodbc://\", creator=get_connection)\n",
    "\n",
    "    # 5. Handshake\n",
    "    print(\"‚è≥ Testing connection (Handshake)...\")\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(\"SELECT @@VERSION\"))\n",
    "        version = result.fetchone()[0]\n",
    "        print(\"\\n‚úÖ SUCCESS! Connection Verified.\")\n",
    "        print(f\"   Server Version: {version.splitlines()[0]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå CONNECTION FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### üì• Step: Loading the \"Silver\" Data\n",
    "\n",
    "### **Architecture Insight: The Medallion Model**\n",
    "In Data Engineering, best practice is the **Medallion Architecture**:\n",
    "* **ü•â Bronze Layer (Raw):** Raw `.tar.gz` files sitting in a folder. They are hard to query and \"messy.\"\n",
    "* **ü•à Silver Layer (Clean):** (Currently here) Clean the headers, and organize them into **Delta Tables** (high-performance SQL tables).\n",
    "* **ü•á Gold Layer (Curated):** Aggregated data ready for dashboards and AI agents.\n",
    "\n",
    "We will query the `Clinical_Patients_Silver` table we created in the previous PySpark notebook.\n",
    "\n",
    "**Note:** For this analysis, we are fetching the top 1000 rows. In a production scenario with millions of rows, we would perform the aggregation (SUM/AVG) on the server side using SQL before bringing data into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the Delta Table\n",
    "query = \"SELECT TOP 1000 * FROM clinical_patients_silver\"\n",
    "\n",
    "try:\n",
    "    print(\"running query...\")\n",
    "    df = pd.read_sql(query, engine)\n",
    "    \n",
    "    print(f\"‚úÖ Data Loaded Successfully.\")\n",
    "    print(f\"   Rows: {df.shape[0]}\")\n",
    "    print(f\"   Columns: {df.shape[1]}\")\n",
    "    \n",
    "    # Preview the data\n",
    "    display(df.head())\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Query Failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### üìä Step: Statistical Health Check\n",
    "\n",
    "Before making charts, we look at the raw numbers. We use `describe()` to get a high-level summary.\n",
    "\n",
    "**What we are looking for:**\n",
    "* **Missing Values:** Do we have `NaN` in critical columns like `AGE` or `OS_MONTHS`?\n",
    "* **Impossible Min/Max:** Is the minimum Age negative? Is the max Age 200?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numerical columns\n",
    "display(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n--- Missing Value Count ---\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### üïµÔ∏è‚Äç‚ôÄÔ∏è Step: Detecting \"Artificial Data Ceilings\"\n",
    "\n",
    "**The Theory (Right-Censoring):**\n",
    "In data science, \"Censoring\" happens when our measurement tool has a limit.\n",
    "* **The Analogy:** Imagine measuring patients with a stopwatch that automatically stops at **60 months**. If a patient survives for 100 months, the dataset incorrectly records them as \"60.\"\n",
    "* **The Risk:** If we train an AI on this, it learns a false physical law: *\"It is impossible for any human to survive longer than 5 years.\"* This effectively ruins the model's ability to predict long-term outcomes.\n",
    "\n",
    "**The Detection:**\n",
    "We are looking for a **\"Wall\"**‚Äîan unnatural vertical spike at the very end of the histogram.\n",
    "* **If we see a spike:** The data is \"Capped\" and needs special handling.\n",
    "* **If we see a smooth tail:** The data is \"Organic\" and high-quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the visual style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create a Histogram for Overall Survival Months\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df['OS_MONTHS'], kde=True, bins=40, color=\"teal\")\n",
    "\n",
    "plt.title(\"Distribution of Overall Survival (Months)\", fontsize=16)\n",
    "plt.xlabel(\"Months\", fontsize=12)\n",
    "plt.ylabel(\"Patient Count\", fontsize=12)\n",
    "\n",
    "# Highlight the potential cap (adjust the '60' if your data shows a different max)\n",
    "max_val = df['OS_MONTHS'].max()\n",
    "plt.axvline(max_val, color='red', linestyle='--', linewidth=2, label=f'Max Value ({max_val})')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Observation: The maximum recorded survival is {max_val} months.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### üìâ Step: Detecting Outliers (Boxplots)\n",
    "\n",
    "**The Theory:**\n",
    "Outliers are data points that differ significantly from other observations. In healthcare, they can be legitimate (rare cases) or errors (typos).\n",
    "* **The Tool:** A **Boxplot** is the industry standard for this.\n",
    "* **The Interpretation:** The \"Box\" holds the middle 50% of patients. The \"Whiskers\" extend to normal limits. Any dots *outside* the whiskers are potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create side-by-side boxplots for Age and Survival\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Age\n",
    "sns.boxplot(x=df['AGE'], ax=axes[0], color=\"skyblue\")\n",
    "axes[0].set_title(\"Age Distribution (Checking for Entry Errors)\")\n",
    "\n",
    "# Plot 2: Survival Months\n",
    "sns.boxplot(x=df['OS_MONTHS'], ax=axes[1], color=\"salmon\")\n",
    "axes[1].set_title(\"Survival Distribution (Checking for skew)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## üìù Conclusion & Technical Decisions\n",
    "\n",
    "Through this analysis, we have performed a \"Health Check\" on our OneLake data and reached the following determinations:\n",
    "\n",
    "1.  **Architecture Validation:** Confirmed that the `PySpark` pipeline correctly transformed raw `.tar.gz` archives into a queryable `Silver` Delta Table (N=577 records).\n",
    "2.  **Security Verification:** Validated `ActiveDirectoryInteractive` connectivity, ensuring no SQL credentials are hardcoded in the codebase.\n",
    "3.  **Preprocessing Strategy:**\n",
    "    * **Capping:** No artificial 60-month wall was detected in `OS_MONTHS`, meaning the survival data is organic and does not require right-censoring correction.\n",
    "    * **Outliers:** Detected outliers in the `AGE` distribution. **Decision:** We will use `RobustScaler` (instead of Standard Scaler) in the inference pipeline to minimize the impact of these extreme values.\n",
    "\n",
    "### **üëâ Next Objective: Phase 3 (AI Guardrails)**\n",
    "Now that the data backend is validated, we must secure the **Serving Layer**.\n",
    "We will build the **Safety Middleware** (`safety_middleware/guardrails.py`) to prevent PII leakage and enforce policy compliance before allowing the AI Agent to query this data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
