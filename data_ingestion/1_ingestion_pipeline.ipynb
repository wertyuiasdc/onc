{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# üß¨ Project: Oncology Clinical Intelligence Agent (Backend Logic)\n",
    "**Author:** Carnegie Johnson | Consultant / AI Engineer / Instructor  \n",
    "**Phase:** 1 - Data Ingestion & Environment Setup\n",
    "\n",
    "### **üëã Welcome!**\n",
    "This notebook serves as the \"Engine Room\" for a prototype data analytics platform. The goal is to build an automated pipeline that fetches/downloads public cancer research data from public sources like **cBioPortal** and securely store the raw data in our private **Microsoft OneLake**.\n",
    "\n",
    "If you are new to Azure, Python, Visual Studio Code, or Jupyter notebooks don't worry! We are here to learn together and *enjoy* what we are doing. I welcome feedback and questions.\n",
    "\n",
    "### **Data Architecture Overview**\n",
    "Using \"Best Practices\" in the Microsoft Fabric ecosystem, we follow the Medallion Architecture\n",
    "1.  **Source:** Public Internet for our ü•â Bronze Layer (cBioPortal, etc.)\n",
    "2.  **Processing:** Local Python Logic (In-Memory Decompression)\n",
    "3.  **Destination:** Microsoft OneLake (The \"OneDrive\" for our data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Install Libraries\n",
    "Instead of a requirements.txt file, install packages directly in the first cell to ensure kernel has them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv azure-identity azure-storage-file-datalake requests tqdm pandas seaborn matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Azure login if not already logged in (Interactive)\n",
    "!az login\n",
    "3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### üîé Discovery Step 1: Find your Workspace ID\n",
    "\n",
    "Before we can create or access a Lakehouse, we need to know the unique ID of the Microsoft Fabric Workspace.\n",
    "\n",
    "**Note:**\n",
    "Since Microsoft Fabric is relatively new, standard Azure CLI commands often don't cover it yet. In some cases we use pure Python or `az rest` to send a direct \"GET\" or \"POST\" request to the **Fabric API**. \n",
    "\n",
    "The following code cell asks Microsoft: *\"Please show me a table of all workspaces this user has access to.\"*\n",
    "\n",
    "> **üìù Action Item:** Look at the output below. Copy the **ID** string for the workspace you want to use. You will need to paste this GUID value into your `.env` file assigned to `AZURE_WORKSPACE_ID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# List all Fabric Workspaces\n",
    "# We add --resource to tell the CLI we need a token for the Fabric API\n",
    "!az rest --method get \\\n",
    "    --url \"https://api.fabric.microsoft.com/v1/workspaces\" \\\n",
    "    --resource \"https://api.fabric.microsoft.com\" \\\n",
    "    --query \"value[].{Name:displayName, ID:id, Description:description}\" \\\n",
    "    --output table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### **Optional**\n",
    "Inspect Specific Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 1. Force reload the .env file\n",
    "# override=True is critical; otherwise, it ignores changes if the var is already set\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# ‚ö†Ô∏è ACTION REQUIRED: Paste the Workspace ID you found above\n",
    "target_workspace_id = \"PASTE_YOUR_GUID_HERE\"  # os.getenv(\"AZURE_WORKSPACE_ID\")\n",
    "api_url = f\"https://api.fabric.microsoft.com/v1/workspaces/{target_workspace_id}/items\"\n",
    "if target_workspace_id != \"PASTE_YOUR_GUID_HERE\":\n",
    "    print(f\"Inspecting Workspace: {target_workspace_id}...\")\n",
    "    \n",
    "    command = (\n",
    "        f'az rest --method get '\n",
    "        f'--url \"{api_url}\" '\n",
    "        f'--resource \"https://api.fabric.microsoft.com\" '\n",
    "        f'--query \"value[?type==\\'Lakehouse\\'].{{Name:displayName, ID:id}}\" '\n",
    "        f'--output table'\n",
    "    )\n",
    "    exit_code = os.system(command)\n",
    "    if exit_code == 0:\n",
    "        print(\"\\n‚úÖ Success\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Failed\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please update the 'target_workspace_id' variable above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Create the Lakehouse\n",
    "Execute the Azure CLI command to create our lakehouse if needed. \n",
    "\n",
    "1.  **Payload Check:** Add `\"type\": \"Lakehouse\"` to the JSON body (required by Microsoft).\n",
    "2.  **Auto-Execution:** Use Python's `subprocess` tool to actually run the command.\n",
    "3.  **Quote Safety:** Let Python handle the JSON formatting so we don't have to worry about escaping quotes manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.exceptions import ClientAuthenticationError\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()    # loads .env from the notebook's working dir (or project root)\n",
    "\n",
    "def smart_create_lakehouse():\n",
    "    # --- 1. CONFIGURATION\n",
    "    ws_id = os.getenv(\"AZURE_WORKSPACE_ID\")\n",
    "    lh_nm = os.getenv(\"LAKEHOUSE_NAME\")\n",
    "    \n",
    "    if not ws_id or not lh_nm:\n",
    "        print(\"‚ùå Error: Missing configuration. Check your .env file.\")\n",
    "        return\n",
    "\n",
    "    print(f\"‚öôÔ∏è  Configuration Loaded.\")\n",
    "    print(f\"   - Workspace ID: {ws_id}\")\n",
    "    print(f\"   - Target Lakehouse: {lh_nm}\")\n",
    "\n",
    "    # --- 2. AUTHENTICATION ---\n",
    "    print(\"\\nüîë Authenticating...\")\n",
    "    cred = DefaultAzureCredential()\n",
    "    try:\n",
    "        # Request token specifically for Fabric API\n",
    "        token = cred.get_token(\"https://api.fabric.microsoft.com/.default\")\n",
    "    except ClientAuthenticationError:\n",
    "        print(\"‚ùå Authentication failed. Please run '!az login' in a separate cell.\")\n",
    "        return\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token.token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # --- 3. PRE-FLIGHT CHECK: FABRIC CAPACITY ---\n",
    "    print(\"üîç Verifying Workspace Capacity...\")\n",
    "    ws_url = f\"https://api.fabric.microsoft.com/v1/workspaces/{ws_id}\"\n",
    "    \n",
    "    ws_response = requests.get(ws_url, headers=headers)\n",
    "    \n",
    "    if ws_response.status_code != 200:\n",
    "        print(f\"‚ùå Failed to fetch workspace details (Status: {ws_response.status_code})\")\n",
    "        print(ws_response.text)\n",
    "        return\n",
    "\n",
    "    ws_data = ws_response.json()\n",
    "    capacity_id = ws_data.get('capacityId')\n",
    "    \n",
    "    if not capacity_id:\n",
    "        print(\"\\nüõë CRITICAL BLOCKER: No Capacity Assigned\")\n",
    "        print(f\"   The workspace '{ws_data.get('displayName')}' is not assigned to a Fabric Capacity.\")\n",
    "        print(\"   -> Action: Go to Fabric Portal -> Workspace Settings -> Premium/Fabric Capacity -> Assign a Trial or F-SKU.\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"‚úÖ Capacity Verified (ID: {capacity_id})\")\n",
    "\n",
    "    # --- 4. IDEMPOTENCY CHECK (Does it exist?) ---\n",
    "    items_url = f\"https://api.fabric.microsoft.com/v1/workspaces/{ws_id}/items\"\n",
    "    items_response = requests.get(items_url, headers=headers)\n",
    "    \n",
    "    if items_response.status_code == 200:\n",
    "        existing_items = items_response.json().get('value', [])\n",
    "        for item in existing_items:\n",
    "            if item['displayName'] == lh_nm and item['type'] == 'Lakehouse':\n",
    "                print(f\"\\n‚úÖ Lakehouse '{lh_nm}' already exists (ID: {item['id']}). Skipping creation.\")\n",
    "                return\n",
    "\n",
    "    # --- 5. EXECUTION (Create Lakehouse) ---\n",
    "    print(f\"\\nüî® Creating Lakehouse '{lh_nm}'...\")\n",
    "    payload = {\n",
    "        \"displayName\": lh_nm,\n",
    "        \"type\": \"Lakehouse\"\n",
    "    }\n",
    "    \n",
    "    create_response = requests.post(items_url, headers=headers, json=payload)\n",
    "    \n",
    "    if create_response.status_code in [201, 202]:\n",
    "        data = create_response.json()\n",
    "        print(\"üéâ SUCCESS! Lakehouse created successfully.\")\n",
    "        print(f\"   ID: {data.get('id')}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Creation Failed (Status {create_response.status_code})\")\n",
    "        print(f\"   Error Message: {create_response.text}\")\n",
    "\n",
    "# Run the smart function\n",
    "smart_create_lakehouse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Data Ingestion\n",
    "\n",
    "This cell may be used as a template to be customized per data source for ingestion.\n",
    "Site 1: cBioPortal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import logging\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from tqdm import tqdm # Switched to standard tqdm (works everywhere)\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- 1. Configuration & Auth ---\n",
    "# Force reload .env to fix the \"ValueError\" from before\n",
    "load_dotenv(override=True)\n",
    "\n",
    "ws_nm = os.getenv(\"AZURE_WORKSPACE_NAME\") or \"MyWorkspace\" # Fallback if env fails\n",
    "lh_nm = os.getenv(\"LAKEHOUSE_NAME\") or \"MyLakehouse\"\n",
    "ol_ep = os.getenv(\"ONELAKE_ENDPOINT\") or \"https://onelake.dfs.fabric.microsoft.com\"\n",
    "\n",
    "# The Official Source of Truth API\n",
    "CBIOPORTAL_API_STUDIES = \"https://www.cbioportal.org/api/studies\"\n",
    "# The Official S3 Bucket for direct downloads\n",
    "CBIOPORTAL_S3_BASE = \"https://cbioportal-datahub.s3.amazonaws.com\"\n",
    "\n",
    "# Setup Logging\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    filename=f\"logs/ingestion_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\",\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logging.getLogger().addHandler(logging.StreamHandler())\n",
    "\n",
    "print(f\"Connecting to OneLake: {ws_nm} / {lh_nm}\")\n",
    "\n",
    "# Auth\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    service_client = DataLakeServiceClient(account_url=ol_ep, credential=credential)\n",
    "    fs_client = service_client.get_file_system_client(file_system=ws_nm)\n",
    "except Exception as e:\n",
    "    print(f\"Auth failed: {e}\")\n",
    "    # Stop execution if auth fails\n",
    "    raise \n",
    "\n",
    "# --- 2. Helper Functions ---\n",
    "\n",
    "def get_valid_studies():\n",
    "    \"\"\"Fetches the official list of study IDs from the cBioPortal API.\"\"\"\n",
    "    try:\n",
    "        logging.info(\"Fetching study list from API...\")\n",
    "        r = requests.get(CBIOPORTAL_API_STUDIES, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        \n",
    "        # Parse JSON and extract IDs\n",
    "        all_studies = r.json()\n",
    "        \n",
    "        # Create a dictionary of {id: url}\n",
    "        study_map = {}\n",
    "        for s in all_studies:\n",
    "            s_id = s['studyId']\n",
    "            # Construct S3 URL\n",
    "            study_map[s_id] = f\"{CBIOPORTAL_S3_BASE}/{s_id}.tar.gz\"\n",
    "            \n",
    "        logging.info(f\"API returned {len(study_map)} studies.\")\n",
    "        return study_map\n",
    "    except Exception as e:\n",
    "        logging.error(f\"API Error: {e}\")\n",
    "        return {}\n",
    "\n",
    "def upload_file_to_onelake(target_path, data_bytes):\n",
    "    \"\"\"Uploads bytes to OneLake.\"\"\"\n",
    "    try:\n",
    "        full_path = f\"{lh_nm}.Lakehouse/Files/{target_path}\"\n",
    "        file_client = fs_client.get_file_client(full_path)\n",
    "        file_client.create_file()\n",
    "        file_client.upload_data(data_bytes, overwrite=True)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Upload Error ({target_path}): {e}\")\n",
    "        return False\n",
    "\n",
    "def process_study(study_id, url):\n",
    "    \"\"\"Downloads from S3 and uploads to OneLake.\"\"\"\n",
    "    try:\n",
    "        # Check if file exists (HEAD request)\n",
    "        head = requests.head(url, timeout=5)\n",
    "        if head.status_code != 200:\n",
    "            logging.warning(f\"Skipping {study_id}: File not found on S3.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Downloading {study_id}...\")\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            \n",
    "            target_path = f\"Raw/cBioPortal/Archives/{study_id}.tar.gz\"\n",
    "            upload_file_to_onelake(target_path, r.content)\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed {study_id}: {e}\")\n",
    "\n",
    "# --- 3. Execution ---\n",
    "\n",
    "# Get List\n",
    "studies = get_valid_studies()\n",
    "\n",
    "# Filter for a specific type to test (e.g., 'gbm' for Glioblastoma or 'brca' for Breast)\n",
    "# REMOVE the filter to download everything (Warning: 300+ studies!)\n",
    "target_ids = {k: v for k, v in studies.items() if 'gbm' in k.lower()}\n",
    "\n",
    "print(f\"Found {len(target_ids)} matching studies (filtering for 'gbm' for safety) out of {len(studies)} available studies.\")\n",
    "\n",
    "# Run Loop (Using standard tqdm)\n",
    "for s_id, url in tqdm(list(target_ids.items())[:3], desc=\"Ingesting...\"):\n",
    "    process_study(s_id, url)\n",
    "\n",
    "print(\"Done. Check OneLake 'Files/Raw/cBioPortal/Archives'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# üèÅ Phase 1 Complete: Transitioning to the \"Silver Layer\"\n",
    "\n",
    "**Congratulations!**   Pipeline is established that fetches raw data from the public datasets and securely stores it in **OneLake**.\n",
    "\n",
    "### **Architecture Insight: The Medallion Model**\n",
    "In Data Engineering, best practice is the **Medallion Architecture**:\n",
    "* **ü•â Bronze Layer (Raw):** (Currently here) Raw `.tar.gz` files sitting in a folder. They are hard to query and \"messy.\"\n",
    "* **ü•à Silver Layer (Clean):** Next step is to unpack these files, clean the headers, and organize them into **Delta Tables** (high-performance SQL tables).\n",
    "* **ü•á Gold Layer (Curated):** Aggregated data ready for dashboards and AI agents.\n",
    "\n",
    "### **üëâ Next Step: Cloud-Based Processing**\n",
    "To deal with complex compressed files, switch to **Microsoft Fabric Notebooks** (powered by PySpark) to handle the heavy lifting.\n",
    "\n",
    "# ‚ö†Ô∏è ACTION REQUIRED: \n",
    "**Instructions:**\n",
    "1.  Open your browser and go to [Microsoft Fabric](https://app.fabric.microsoft.com).\n",
    "2.  Navigate to your Workspace (`<YOUR_WORKSPACE_NAME>`).\n",
    "3.  Click **+ New Item** -> **Notebook**.\n",
    "4.  Copy the **PySpark Code** noted below and paste it into the first cell of your new online notebook.\n",
    "5.  Run the cell to transform the Raw Files into a Silver Delta Table.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚ö†Ô∏è ACTION REQUIRED: \n",
    "### **üìã Copy-Paste Code for Fabric Notebook**\n",
    "\n",
    "```python\n",
    "# -----------------------------------------------------------------------\n",
    "# PHASE 2: BRONZE TO SILVER TRANSFORMATION\n",
    "# Run this in a Microsoft Fabric Notebook (PySpark Kernel)\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "import tarfile\n",
    "import os\n",
    "from glob import glob\n",
    "from pyspark.sql.functions import input_file_name, regexp_extract, col\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# The \"Lakehouse\" mount point is standard in Fabric\n",
    "BASE_PATH = \"/lakehouse/default/Files\"\n",
    "RAW_PATH = f\"{BASE_PATH}/Raw/cBioPortal\"\n",
    "EXTRACT_PATH = f\"{BASE_PATH}/Staging/Clinical\"\n",
    "\n",
    "# --- STEP 1: UNPACK (Standard Python) ---\n",
    "# Spark cannot read inside .tar.gz files easily, so we extract them first.\n",
    "print(\"üì¶ Step 1: Extracting Clinical Data from Archives...\")\n",
    "os.makedirs(EXTRACT_PATH, exist_ok=True)\n",
    "\n",
    "# Find all uploaded tar.gz files\n",
    "archives = glob(f\"{RAW_PATH}/*/*.tar.gz\")\n",
    "\n",
    "if not archives:\n",
    "    print(\"‚ùå No files found! Did you run the ingestion pipeline?\")\n",
    "else:\n",
    "    for archive in archives:\n",
    "        try:\n",
    "            # Create a readable ID from the folder name\n",
    "            study_id = os.path.basename(os.path.dirname(archive))\n",
    "            \n",
    "            with tarfile.open(archive, \"r:gz\") as tar:\n",
    "                for member in tar.getmembers():\n",
    "                    # We only extract the 'patient' data file\n",
    "                    if \"data_clinical_patient.txt\" in member.name:\n",
    "                        # Rename it to avoid collisions (e.g., study1_patient.txt)\n",
    "                        member.name = f\"{study_id}_patient_data.txt\" \n",
    "                        tar.extract(member, path=EXTRACT_PATH)\n",
    "                        print(f\"   -> Extracted: {member.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error reading {archive}: {e}\")\n",
    "\n",
    "# --- STEP 2: LOAD & CLEAN (PySpark) ---\n",
    "print(\"\\n‚ú® Step 2: Loading into Delta Table...\")\n",
    "\n",
    "# cBioPortal files have comments starting with '#'. We tell Spark to ignore them.\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"comment\", \"#\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(f\"{EXTRACT_PATH}/*.txt\")\n",
    "\n",
    "# Add the 'Study_ID' column by extracting it from the filename\n",
    "df_enriched = df.withColumn(\"SourceFile\", input_file_name()) \\\n",
    "                .withColumn(\"Study_ID\", regexp_extract(col(\"SourceFile\"), r\".*/(.*)_patient_data.txt\", 1)) \\\n",
    "                .drop(\"SourceFile\")\n",
    "\n",
    "# Clean Column Names (Remove spaces/parentheses for database compatibility)\n",
    "for name in df_enriched.columns:\n",
    "    clean_name = name.replace(\" \", \"_\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    df_enriched = df_enriched.withColumnRenamed(name, clean_name)\n",
    "\n",
    "# --- STEP 3: WRITE TO SILVER ---\n",
    "table_name = \"Clinical_Patients_Silver\"\n",
    "df_enriched.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"\\nüéâ Success! Data saved to Delta Table: '{table_name}'\")\n",
    "print(f\"üìä Total Records: {df_enriched.count()}\")\n",
    "\n",
    "# Show a sample\n",
    "display(df_enriched.limit(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
