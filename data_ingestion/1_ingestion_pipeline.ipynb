{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f0c4774",
   "metadata": {},
   "source": [
    "# üß¨ Project: Oncology Clinical Intelligence Agent (Backend Logic)\n",
    "**Author:** Carnegie Johnson | Consultant / AI Engineer / Instructor  \n",
    "**Phase:** 1 - Data Ingestion & Environment Setup\n",
    "\n",
    "### **üëã Welcome!**\n",
    "This notebook serves as the \"Engine Room\" for a prototype data analytics platform. The goal is to build an automated pipeline that fetches/downloads public cancer research data from public sources like **cBioPortal** and securely store the raw data it in our private **Microsoft OneLake**.\n",
    "\n",
    "If you are new to Azure, Python, Visual Studio Code, or Jupyter notebooks don't worry! We are here to learn together and *enjoy* what we are doing. I welcome feedback and questions.\n",
    "\n",
    "### **Data Architecture Overview**\n",
    "Using \"Best Practices\" in the Microsoft Fabric ecosystem, we follow the Medallion Architecture\n",
    "1.  **Source:** Public Internet for our ü•â Bronze Layer (cBioPortal, etc.)\n",
    "2.  **Processing:** Local Python Logic (In-Memory Decompression)\n",
    "3.  **Destination:** Microsoft OneLake (The \"OneDrive\" for our data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb02436b",
   "metadata": {},
   "source": [
    "### üîé Discovery Step 1: Find your Workspace ID\n",
    "\n",
    "Before we can create or access a Lakehouse, we need to know the unique ID of the Microsoft Fabric Workspace.\n",
    "\n",
    "**Note:**\n",
    "Since Microsoft Fabric is relatively new, standard Azure CLI commands often don't cover it yet. In some cases We use pure Python or `az rest` to send a direct \"GET\" or \"POST\" request to the **Fabric API**. \n",
    "\n",
    "The following code cell asks Microsoft: *\"Please show me a table of all workspaces this user has access to.\"*\n",
    "\n",
    "> **üìù Action Item:** Look at the output below. Copy the **ID** string for the workspace you want to use. You will need to paste this GUID value into your `.env` file assigned to `AZURE_WORKSPACE_ID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ab099",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# List all Fabric Workspaces\n",
    "# We add --resource to tell the CLI we need a token for the Fabric API\n",
    "!az rest --method get \\\n",
    "    --url \"https://api.fabric.microsoft.com/v1/workspaces\" \\\n",
    "    --resource \"https://api.fabric.microsoft.com\" \\\n",
    "    --query \"value[].{Name:displayName, ID:id, Description:description}\" \\\n",
    "    --output table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229addc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import subprocess\n",
    "\n",
    "# 1. Force reload the .env file\n",
    "# override=True is critical; otherwise, it ignores changes if the var is already set\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# ‚ö†Ô∏è ACTION REQUIRED: Paste the Workspace ID you found above\n",
    "target_workspace_id = \"PASTE_YOUR_GUID_HERE\"  # os.getenv(\"AZURE_WORKSPACE_ID\")\n",
    "api_url = f\"https://api.fabric.microsoft.com/v1/workspaces/{target_workspace_id}/items\"\n",
    "if target_workspace_id != \"PASTE_YOUR_GUID_HERE\":\n",
    "    print(f\"Inspecting Workspace: {target_workspace_id}...\")\n",
    "    \n",
    "    cmd = [\n",
    "        \"az\",\n",
    "        \"rest\",\n",
    "        \"--method\",\n",
    "        \"get\",\n",
    "        \"--url\",\n",
    "        api_url,\n",
    "        \"--resource\",\n",
    "        \"https://api.fabric.microsoft.com\",\n",
    "        \"--query\",\n",
    "        \"value[?type=='Lakehouse'].{Name:displayName, ID:id}\",\n",
    "        \"--output\",\n",
    "        \"table\",\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(cmd, check=False)\n",
    "        if result.returncode == 0:\n",
    "            print(\"\\n‚úÖ Success\")\n",
    "        else:\n",
    "            print(\"\\n‚ùå Failed\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Failed to execute 'az rest': {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please update the 'target_workspace_id' variable above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425e90d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import logging\n",
    "from typing import Dict\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from tqdm import tqdm # Switched to standard tqdm (works everywhere)\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- 1. Configuration & Auth ---\n",
    "# Force reload .env to fix the \"ValueError\" from before\n",
    "load_dotenv(override=True)\n",
    "\n",
    "ws_nm = os.getenv(\"AZURE_WORKSPACE_NAME\") or \"MyWorkspace\" # Fallback if env fails\n",
    "lh_nm = os.getenv(\"LAKEHOUSE_NAME\") or \"MyLakehouse\"\n",
    "ol_ep = os.getenv(\"ONELAKE_ENDPOINT\") or \"https://onelake.dfs.fabric.microsoft.com\"\n",
    "\n",
    "# The Official Source of Truth API\n",
    "CBIOPORTAL_API_STUDIES = \"https://www.cbioportal.org/api/studies\"\n",
    "# The Official S3 Bucket for direct downloads\n",
    "CBIOPORTAL_S3_BASE = \"https://cbioportal-datahub.s3.amazonaws.com\"\n",
    "\n",
    "# Setup Logging\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    filename=f\"logs/ingestion_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\",\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "# Avoid adding multiple StreamHandlers if this cell is re-run\n",
    "root_logger = logging.getLogger()\n",
    "if not any(isinstance(h, logging.StreamHandler) for h in root_logger.handlers):\n",
    "    root_logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "print(f\"Connecting to OneLake: {ws_nm} / {lh_nm}\")\n",
    "\n",
    "# Auth\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    service_client = DataLakeServiceClient(account_url=ol_ep, credential=credential)\n",
    "    fs_client = service_client.get_file_system_client(file_system=ws_nm)\n",
    "except Exception as e:\n",
    "    print(f\"Auth failed: {e}\")\n",
    "    # Stop execution if auth fails\n",
    "    raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f78b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Helper Functions ---\n",
    "\n",
    "def get_valid_studies() -> Dict[str, str]:\n",
    "    \"\"\"Fetches the official list of study IDs from the cBioPortal API.\"\"\"\n",
    "    try:\n",
    "        logging.info(\"Fetching study list from API...\")\n",
    "        r = requests.get(CBIOPORTAL_API_STUDIES, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        \n",
    "        # Parse JSON and extract IDs\n",
    "        all_studies = r.json()\n",
    "        \n",
    "        # Create a dictionary of {id: url}\n",
    "        study_map: Dict[str, str] = {}\n",
    "        for s in all_studies:\n",
    "            s_id = s['studyId']\n",
    "            # Construct S3 URL\n",
    "            study_map[s_id] = f\"{CBIOPORTAL_S3_BASE}/{s_id}.tar.gz\"\n",
    "            \n",
    "        logging.info(f\"API returned {len(study_map)} studies.\")\n",
    "        return study_map\n",
    "    except Exception as e:\n",
    "        logging.error(f\"API Error: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def upload_file_to_onelake(target_path: str, data_bytes: bytes) -> bool:\n",
    "    \"\"\"Uploads bytes to OneLake.\"\"\"\n",
    "    try:\n",
    "        full_path = f\"{lh_nm}.Lakehouse/Files/{target_path}\"\n",
    "        file_client = fs_client.get_file_client(full_path)\n",
    "        file_client.create_file()\n",
    "        file_client.upload_data(data_bytes, overwrite=True)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Upload Error ({target_path}): {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def process_study(study_id: str, url: str) -> None:\n",
    "    \"\"\"Downloads from S3 and uploads to OneLake.\"\"\"\n",
    "    try:\n",
    "        # Check if file exists (HEAD request)\n",
    "        head = requests.head(url, timeout=5)\n",
    "        if head.status_code != 200:\n",
    "            logging.warning(f\"Skipping {study_id}: File not found on S3.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Downloading {study_id}...\")\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            \n",
    "            target_path = f\"Raw/cBioPortal/Archives/{study_id}.tar.gz\"\n",
    "            upload_file_to_onelake(target_path, r.content)\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed {study_id}: {e}\")\n",
    "\n",
    "# --- 3. Execution ---\n",
    "\n",
    "# Get List\n",
    "studies = get_valid_studies()\n",
    "\n",
    "# Filter for a specific type to test (e.g., 'gbm' for Glioblastoma or 'brca' for Breast)\n",
    "# REMOVE the filter to download everything (Warning: 300+ studies!)\n",
    "target_ids = {k: v for k, v in studies.items() if 'gbm' in k.lower()}\n",
    "\n",
    "print(f\"Found {len(target_ids)} matching studies (filtering for 'gbm' for safety) out of {len(studies)} available studies.\")\n",
    "\n",
    "# Run Loop (Using standard tqdm)\n",
    "for s_id, url in tqdm(list(target_ids.items())[:3], desc=\"Ingesting...\"):\n",
    "    process_study(s_id, url)\n",
    "\n",
    "print(\"Done. Check OneLake 'Files/Raw/cBioPortal/Archives'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c01f64",
   "metadata": {},
   "source": [
    "### ** Optional **\n",
    "Inspect Specific Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dcd1e6",
   "metadata": {},
   "source": [
    "### Create the Lakehouse\n",
    "Execute the lakehouse creation command if needed. \n",
    "\n",
    "1.  **Payload Check:** Add `\"type\": \"Lakehouse\"` to the JSON body (required by Microsoft).\n",
    "2.  **Auto-Execution:** Use Python's `subprocess` tool to actually run the command.\n",
    "3.  **Quote Safety:** Let Python handle the JSON formatting so we don't have to worry about escaping quotes manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf6415",
   "metadata": {},
   "source": [
    "### Data Ingestion\n",
    "\n",
    "This cell may be used as a template to be customized per data source for ingestion.\n",
    "Site 1: cBioPortal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3fdfc0",
   "metadata": {},
   "source": [
    "# üèÅ Phase 1 Complete: Transitioning to the \"Silver Layer\"\n",
    "\n",
    "**Congratulations!**   Pipeline established that fetches raw data from the public datasets and securely stores it in **OneLake**.\n",
    "\n",
    "### **Architecture Insight: The Medallion Model**\n",
    "In Data Engineering, best practice is the **Medallion Architecture**:\n",
    "* **ü•â Bronze Layer (Raw):** (Currently here) Raw `.tar.gz` files sitting in a folder. They are hard to query and \"messy.\"\n",
    "* **ü•à Silver Layer (Clean):** Next step is to unpack these files, clean the headers, and organize them into **Delta Tables** (high-performance SQL tables).\n",
    "* **ü•á Gold Layer (Curated):** Aggregated data ready for dashboards and AI agents.\n",
    "\n",
    "### **üëâ Next Step: Cloud-Based Processing**\n",
    "To deal with complex compressed files, switch to **Microsoft Fabric Notebooks** (powered by PySpark) to handle the heavy lifting.\n",
    "\n",
    "# ‚ö†Ô∏è ACTION REQUIRED: \n",
    "**Instructions:**\n",
    "1.  Open your browser and go to [Microsoft Fabric](https://app.fabric.microsoft.com).\n",
    "2.  Navigate to your Workspace (`<YOUR_WORKSPACE_NAME>`).\n",
    "3.  Click **+ New Item** -> **Notebook**.\n",
    "4.  Copy the **PySpark Code** noted below and paste it into the first cell of your new online notebook.\n",
    "5.  Run the cell to transform the Raw Files into a Silver Delta Table.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚ö†Ô∏è ACTION REQUIRED: \n",
    "### **üìã Copy-Paste Code for Fabric Notebook**\n",
    "\n",
    "```python\n",
    "# -----------------------------------------------------------------------\n",
    "# PHASE 2: BRONZE TO SILVER TRANSFORMATION\n",
    "# Run this in a Microsoft Fabric Notebook (PySpark Kernel)\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "import tarfile\n",
    "import os\n",
    "from glob import glob\n",
    "from pyspark.sql.functions import input_file_name, regexp_extract, col\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# The \"Lakehouse\" mount point is standard in Fabric\n",
    "BASE_PATH = \"/lakehouse/default/Files\"\n",
    "RAW_PATH = f\"{BASE_PATH}/Raw/cBioPortal\"\n",
    "EXTRACT_PATH = f\"{BASE_PATH}/Staging/Clinical\"\n",
    "\n",
    "# --- STEP 1: UNPACK (Standard Python) ---\n",
    "# Spark cannot read inside .tar.gz files easily, so we unzip them first.\n",
    "print(\"üì¶ Step 1: Extracting Clinical Data from Archives...\")\n",
    "os.makedirs(EXTRACT_PATH, exist_ok=True)\n",
    "\n",
    "# Find all uploaded tar.gz files\n",
    "archives = glob(f\"{RAW_PATH}/*/*.tar.gz\")\n",
    "\n",
    "if not archives:\n",
    "    print(\"‚ùå No files found! Did you run the ingestion pipeline?\")\n",
    "else:\n",
    "    for archive in archives:\n",
    "        try:\n",
    "            # Create a readable ID from the folder name\n",
    "            study_id = os.path.basename(os.path.dirname(archive))\n",
    "            \n",
    "            with tarfile.open(archive, \"r:gz\") as tar:\n",
    "                for member in tar.getmembers():\n",
    "                    # We only extract the 'patient' data file\n",
    "                    if \"data_clinical_patient.txt\" in member.name:\n",
    "                        # Rename it to avoid collisions (e.g., study1_patient.txt)\n",
    "                        member.name = f\"{study_id}_patient_data.txt\" \n",
    "                        tar.extract(member, path=EXTRACT_PATH)\n",
    "                        print(f\"   -> Extracted: {member.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error reading {archive}: {e}\")\n",
    "\n",
    "# --- STEP 2: LOAD & CLEAN (PySpark) ---\n",
    "print(\"\\n‚ú® Step 2: Loading into Delta Table...\")\n",
    "\n",
    "# cBioPortal files have comments starting with '#'. We tell Spark to ignore them.\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"comment\", \"#\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(f\"{EXTRACT_PATH}/*.txt\")\n",
    "\n",
    "# Add the 'Study_ID' column by extracting it from the filename\n",
    "df_enriched = df.withColumn(\"SourceFile\", input_file_name()) \\\n",
    "                .withColumn(\"Study_ID\", regexp_extract(col(\"SourceFile\"), r\".*/(.*)_patient_data.txt\", 1)) \\\n",
    "                .drop(\"SourceFile\")\n",
    "\n",
    "# Clean Column Names (Remove spaces/parentheses for database compatibility)\n",
    "for name in df_enriched.columns:\n",
    "    clean_name = name.replace(\" \", \"_\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    df_enriched = df_enriched.withColumnRenamed(name, clean_name)\n",
    "\n",
    "# --- STEP 3: WRITE TO SILVER ---\n",
    "table_name = \"Clinical_Patients_Silver\"\n",
    "df_enriched.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"\\nüéâ Success! Data saved to Delta Table: '{table_name}'\")\n",
    "print(f\"üìä Total Records: {df_enriched.count()}\")\n",
    "\n",
    "# Show a sample\n",
    "display(df_enriched.limit(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
